\documentclass[a4paper,12pt]{book}
%\usepackage{fancyvrb,relsize}
\usepackage[small,compact]{titlesec}
\usepackage[pdftex]{graphicx}
\usepackage{listings}
\lstset{language=C}
%\usepackage[margin=3.50cm]{geometry}
%\linespread{1.5}
%\setlength{\parindent}{0pt}
\setlength{\parskip}{0.85ex plus 0.65ex minus 0.3ex}
\sloppy
%\setlength{\oddsidemargin}{0in} \setlength{\evensidemargin}{0in}
%\setlength{\textwidth}{6.5in} \setlength{\textheight}{9.5in}
%\setlength{\topmargin}{-0.65in}

\begin{document}

%\begin{center}
\chapter{Background}
%{\bf {\LARGE Chapter 1}\linebreak\linebreak{Background}}
%\linebreak
%\linebreak
%\end{center}

\section{Parallelism in Programs}
These days it is hard to find somebody using a computer with single-core processor.
With the help of multi-core and multi-processor machines it is possible to speed up 
the program by mapping the sections of the program to available processors(Remark - 
through out this document the term processor is used interchangeably with core). This 
is generally termed as parallelism in programs. It is very difficult to parallelize
the entire program though. The degree of parallelism is limited by certain factors which is
explained later in this section. In addition this section discusses various types of parallelism and
make a comparison of various approaches towards parallelism which can be applied to programs.

Parallelism and locality

\textbf{POSIX Threads}/Pthreads provides a standard interface for performing mulithreaded computation. 
Threads are subprocesses running with in a process. 
We can find many applications such as a web browser which can take advantage of multithreading.
The efficiency of an application improves when it is designed with threads because they have their
own stack and status. The overhead of creating a separate process can be avoided here.
Resources like files are shared among threads. Though Pthreads are good alternatives for
having multiple processes in a single processor machine it is very difficult to scale
it to multi-core processors. Another limitation of Pthreads is programmers are required to
deal with a lot of thread-specific code. The number of theads required for a computation
need to be hard corded which makes it less scalable.

\textbf{MPI}

\textbf{OpenMP}
In view of the shortcomings of POSIX threads there was an urge to formulate a new threading
interface. The major objective was to overcome the burden of learning different ways for programming threads in different
operating systems with in different programming languages. OpenMP is able to deal with this
by a great extend. As the framework is evolved rather than its APIs, support for pragmas became the distinguished
feature of OpenMP. The user has to specify only the blocks of code that need to be run
as parallel. The compiler does the rest. It will take care of making the pragma annotated blocks into
threads and necessary APIs are inserted to map those into different cores. The example below
shows usage of pragma.

{\footnotesize
\begin{lstlisting}
  #pragma omp parallel for
  for (i = 1; i <= N; i++)
      A[i] = B[i] + c[i]
\end{lstlisting}
}

\textbf{OpenCL}

\textbf{Intel TBB}

\section{Auto Parallelization}
We can take the advantage of hardware support for parallelism only if the compiler has
support for generating the parallel code. There are interfaces like OpenMP for
developing parallel applications. But the user has to manually provide the annotations
for it in the source code. This becomes a tedious task for the user and he has to
ensure the correctness of the code too. This prompted researchers to explore
mechanisms for finding out the parallel portions of the code without manual intervention.

It can be noticed that most of the execution time of a program is spend inside some
for loop. Parallelizing compiler tries to split up a loop so that its iterations can
be executed on separate processors concurrently. A dependency analysis pass is 
performed on the code to determine whether it can be safely parallelized. The following
example illustrates this.

{\footnotesize
\begin{lstlisting}
  for (i = 1; i <= N; i++)
      A[i] = B[i] + c[i]
\end{lstlisting}
}

The analysis detects that there is no dependency between two consecutive iterations and
can be safely parallelized. Consider another example

{\footnotesize
\begin{lstlisting}
  for (i = 2; i <= N; i++)
      A[i] = A[i-1] * 2;
\end{lstlisting}
}

Here a particular iteration is dependent on previous one and so its not safe to parallelize.
An intelligent compiler can convert this into parallel as follows.

{\footnotesize
\begin{lstlisting}
  for (i = 1; i <= N; i++)
      A[i] = A[1] * 2 ** (i - 1);
\end{lstlisting}
}

Detecting this kind of opportunities for parallelization and applying automatic transformation
is a tedious task for existing compilers. A powerful mathematical model explained in the next
section act as a helping hand for the compilers to do such transformations with some
restrictions applied on the input.

\section{The Polyhedral Model}

Polyhedral optimizations are used for many kind of memory access optimization by
looking into the memory access pattern of any piece of code. Any kind of classical
 loop optimization techniques like tiling can be used for this purpose. 



\section{LLVM}
LLVM defines a common, low-level code representation in Static Single Assignment
(SSA) form, with several novel features. The LLVM compiler framework and code
representation together provide a combination of key capabilities that are
important for practical, lifelong analysis and transformation of programs.
One of the important features of LLVM is that the output of all the
transformation passes have same intermediate representation(LLVM IR), which
makes the programmerâ€™s life simple.

\section{Polly}
The framework discussed in this document is implemented using Polly[polly],
an open source[licence] compiler  optimization framework that uses a mathematical
 representation, the polyhedral model, to represent and transform loops and other
 control flow structures. It is an effort towards achieving autoparallelism in programs.
 The transformations are being implemented in LLVM(Low level virtual machine). 
Polly can detect parallel loops, issue vector instructions and generate OpenMP code(focus of 
this document) corresponding to those loops. Polly try to expose more parallelism
with the help of polyhedral model. A loop which does not look parallel can be transformed
to a parallel loop and these can be vectorized or parallelize using OpenMP.

More details on LLVM and Polly can be found at chapters 2 and 3 respectively.

\section{Manual OpenMP Code Generation}

\section{SPEC2006 Benchmarks}
\end{document}
