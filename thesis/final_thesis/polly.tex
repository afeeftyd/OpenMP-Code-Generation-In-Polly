\label{chap:polly}

This chapter deals with the internals of  Polly - Polyhedral Optimization
in LLVM\footnote{\url{http://llvm.org}}.

\section{Introduction to LLVM}

LLVM(Low level virtual machine)\cite{Lattner} is a collection of reusable compiler toolchain technologies that 
allows sophisticated program transformations to be performed with much ease. The
major feature of LLVM is its low level intermediate representation
(LLVM-IR) 
which captures very minute details(like size and type of a variable) of the program code.
Such details would be handy to implement effective optimizations. Tools
are already available to convert programs written in popular programming
languages like C, Python, Java, etc. to LLVM-IR and back. Polly operates
on LLVM-IR, which increases the range of programming languages that gets
benefited. A complete list of documents for LLVM is available here\footnote{\url{http://llvm.org/docs/}}


\section{Introduction to Polly}

Today, effective polyhedral techniques exist to optimize computation intensive
programs.  Advanced data-locality optimizations are available to accelerate
sequential programs \cite{pluto}. Effective methods to expose SIMD and
thread-level parallelism were developed.

Yet, the use of programming-language-specific techniques significantly limits
their impact.  Most polyhedral tools use a basic, language specific front end
to extract relevant code regions. This often requires the source code to be in
a canonical form, disallowing any pointer arithmetic or higher level
language constructs like C++ iterators and prevents the optimization of
programs written in languages like Java or Haskell. Nevertheless, even tools
that limit themselves to a restricted subset of C may apply incorrect
transformations, as the effects of implicit type casts, integer wrapping or
aliasing are mostly ignored. To ensure correctness manual annotation
of code that is regarded safe to optimize is often required.  This prevents
automatic transformations and consequently reduces the impact of existing
tools.

With Polly we are developing a state-of-the-art polyhedral infrastructure for
LLVM, that supports fully automatic transformation of existing programs. Polly
detects and extracts relevant code regions without any human interaction. Since
Polly accepts LLVM-IR as input, it is programming language independent and
transparently supports constructs like C++ iterators, pointer arithmetic or
goto based loops. It is built around an advanced polyhedral library and includes a state-of-the-art
dependency analysis. Due to a simple file interface it is easily possible to
apply transformations manually or to use an external optimizer. We use this
interface to integrate Pluto \cite{pluto}, a modern data locality
optimizer and parallelizer. With integrated SIMD and OpenMP code
generation, Polly automatically takes advantage of existing and newly exposed
parallelism.

This chapter focuses on concepts of Polly which are new or little discussed in the polyhedral community.

\begin{figure}
  \includegraphics[width=1\textwidth]{images/architecture.eps}
  \caption{Architecture of Polly}
  \label{fig:arch}
\end{figure}

\section{Implementation}

Polly is designed as a set of compiler internal analysis and optimization
passes. They can be divided into front end, middle end and back end passes. The
front end translates from LLVM-IR into a polyhedral representation, the middle
end transforms and optimizes this representation and the back end translates it
back to LLVM-IR. In addition, there exist preparing passes to increase the
amount of analyzable code as well as passes to export and reimport the polyhedral
representation.  Figure~\ref{fig:arch} illustrates the overall
architecture.

To optimize a program manually three steps are performed. First of all the
program is translated to LLVM-IR. Afterwards Polly is called to optimize LLVM-IR and
target code is generated. The LLVM-IR representation of a program can
be obtained from language-specific LLVM based compilers. clang is a
good choice. Polly also provides a \texttt{gcc} like user interface that is
called \texttt{pollycc}.

\subsection{LLVM-IR to polyhedral model}

To apply polyhedral optimizations on a program, the first step that needs to be
taken is to find relevant code sections and create a polyhedral description for
them. The code sections that will be optimized by Polly are static
control parts (SCoPs), the classical domain of polyhedral optimizations.

\subsubsection{Region-based SCoP detection}
Polly implements a structured, region-based approach to detect
the SCoPs available in a function. It uses a refined version of the program
structure tree described by Johnson \cite{Johnson}.

A \emph{region} is a subgraph of the control flow graph (CFG) that is connected to the
remaining graph by only two edges, an entry edge and an exit edge.  Viewed as a
unit it does not change control flow. Hence, it can be modelled as a
simple function call, which can easily be replaced with a call to an optimized
version of the function.  A \emph{canonical region} is a region that cannot be
constructed by merging two adjacent smaller regions. A region \emph{contains}
another region if the nodes of one region are a subset of the nodes of the other
region. A tree is called \emph{region tree}, if the nodes of it are canonical regions
and the edges are defined by the contains relation.

To find the SCoPs in a function we look for the maximal regions that are valid
SCoPs. Starting from the outermost region, we look for canonical regions in the
region tree that are valid SCoPs. In case the outermost region is a valid SCoP,
we store it. Otherwise, we check each child. After analyzing the tree, we have a
set of maximal canonical regions that form valid SCoPs. These regions are now
combined to larger non-canonical regions such that the maximal 
non-canonical regions that form valid SCoPs are found.

\subsubsection{Semantic SCoPs}

In contrast to approaches based on the abstract syntax
tree (AST), Polly does not require a SCoP to match any specific syntactic
structure. Instead, it analyzes the semantics of a SCoP. We call SCoPs
that are detected based on semantic criteria \emph{semantic SCoPs}.

A common approach to detect a SCoP is to analyze an AST representation of
the program, that is close to the programming language it is implemented in. In
this AST control flow structures like for loops and conditions are
detected. Then it is checked if they form a SCoP. Refer to Chapter ~\ref{chap:background} for the common restrictions that need
to be met for a SCoP. There are various ways to
extend this definition of a SCoP, which we did not include in this basic
definition.

\begin{figure}
\begin{lstlisting}
                 for (i = 0; i < n + m; i++)
                       A[i] = i;
\end{lstlisting}
	\caption{A valid syntactic SCoP. Not always a valid semantic SCoP}
	\label{fig:syntacticScop}
\end{figure}

The detection of SCoPs as shown in Figure~\ref{fig:syntacticScop} with an AST
based approach is easily possible, however as soon as programs become more
complex and less canonical difficulties arise. The AST of a modern language is
often very expressive, such that there exist numerous ways a program can be
represented.  Sometimes different representations can be canonicalized.
However, as soon as goto based loops should be detected, various induction
variables exist or expressions are spread all over the program, sophisticated
analyses are required to check if a program section is a SCoP. Further
difficulties arise through the large amount of implicit knowledge that is
needed to understand a programming language.  A simple, often overlooked problem is integer
wrapping.  Assuming \texttt{n} and \texttt{m} are unsigned integers of 32 bit
width, it is possible that $n+m < n$ holds. The upper bound in the source code must
therefore be represented as $n+m\bmod 2^{32}$, but no polyhedral tool we know
of models the loop bound in this way. Further problems can be caused by
preprocessor macros, aliasing or C++ (operator) overloading. We believe even
standard C99 is too complex to effectively detect SCoPs in it. Tools like PoCC,
avoids this problem by requiring valid SCoPs to be explicitly annotated in the
source code. However, this prevents any automatic optimization and
significantly limits the impact of polyhedral techniques.

Fortunately, after lowering programs to LLVM-IR the complexity is highly
reduced and constructs like implicit type casts become explicit. Furthermore,
it is possible to run a set of LLVM optimization passes, that further
canonicalize the code.  As a result, an
analysis that detects SCoPs based on their semantics is possible.  LLVM-IR is a
very low-level representation of a program, which does not have loops, but
jumps and gotos and has no arrays or affine expressions, but pointer arithmetic
and three address form operations. From this representation all necessary
information is recomputed using advanced compiler internal analyses available
in LLVM. Simple analyses used are loop detection or dominance information to
verify a SCoP contains only structured control flow. More sophisticated ones
check for aliasing or provide information about side effects of
function calls. 

As Polly successfully recovers all necessary information from a low-level
representation, there are no restrictions on the syntactic structure of the
program source code. A code section is accepted as soon as the LLVM analyses
can prove that it has the semantics of a SCoP. As a result, 
arbitrary control
flow structures are valid if they can be written as a well-structured set
of for-loops and if-conditions with affine expressions in lower and upper
bounds and in the operands of the comparisons. Furthermore, any set of
memory accesses is allowed as long as they behave like array accesses with affine subscripts.
A loop written
with \texttt{do..while} instead of \texttt{for} or fancy pointer arithmetic can easily be part of a valid SCoP.
To illustrate this an example is shown in Figure~\ref{semscop}.

\begin{figure}
\begin{verbatim}
  int A[1024];
  void pointer_loop () {
      int *B = A;
      while (B < &A[1024]) {
          *B = 1;
          ++B;
      }
  }
\end{verbatim}
\caption{Valid semantic SCoPs}
\label{semscop}
\end{figure}

\subsection{Polyhedral model}

\subsubsection{The integer set library} Polly uses isl, an integer set library
developed by Verdoolaege \cite{Verdoolaege10}. Isl natively supports
existentially quantified variables in all its data structures; therefore, Polly
also supports them throughout the whole transformation. This enables Polly to
use accurate operations on $\mathbb{Z}$-polyhedra instead of using polyhedra
in the rationals as approximations of integer sets.
Native support of $\mathbb{Z}$-polyhedra
simplified many internal calculations and we
expect it to be especially useful to represent the modulo semantics of integer
wrapping and type casts.


\subsubsection{Composable polyhedral transformations}
Polly uses the classical polyhedral description \cite{girbal}
that describes a SCoP as a
set of statements each defined by a domain, a schedule and a set of memory
accesses. For more details refer to Chapter ~\ref{chap:background}.

In contrast to most existing tools the domain of a statement cannot be changed
in Polly. All transformations need to be applied on the schedule. There
are two reasons for this. First, we believe it is
conceptually the cleanest approach to use the domain to define the set of
different statement instances that will be executed and to use the schedule for
defining their execution times. As the set of different statement instances
never changes there is no need to change the domain. The second reason is to
obtain compositionality of transformations. As transformations on SCoPs are
described by schedules only, the composition of transformations is simply the
composition of the relations representing the schedules.


\subsubsection{Export/Import}

Polly supports the export and reimport of the polyhedral description.  By
importing an updated description with changed schedules a program can be
transformed easily. To prevent invalid optimizations Polly automatically verifies
newly imported schedules. 
Currently Polly supports the Scoplib exchange format, which is used by
PoCC and Pluto~\cite{pluto}.
Unfortunately, the Scoplib exchange format is not expressive enough to store
information on existentially quantified variables, schedules that include
inequalities or memory accesses that touch more than one element. Therefore, we
have introduced a simple JSON\cite{rfc4627} and isl based exchange format to
experiment with those possibilities.

\subsection{Polyhedral model to LLVM-IR}
Polly uses CLooG \cite{cloog} to translate the polyhedral representation back
into a generic AST. This AST is then
translated into LLVM-IR based loops, conditions and expressions.

\subsubsection{Detecting parallel loops}
Polly can detect parallel loops automatically and generates, if requested,
thread-level parallel code by inserting calls to the GNU OpenMP runtime. This
is targeted to automatically take advantage of parallelism present in the
original code or exposed by previously run optimizers. To ensure correctness of
generated code Polly does not rely on any information provided by external
optimizers, but independently detects parallel loops. We present a novel
approach how to detect them. For details refer to Chapter ~\ref{chap:openmp}


\section{Related work}

The work in Polly was inspired by ideas developed in the Graphite project \cite{TRIFUNOVIC:2010},
yet Polly uses novel approaches in many areas. For instance, Graphite did not
include a structured SCoP detection, even though currently a SCoP detection
similar to the one in Polly is developed.  Furthermore, Graphite works on the
GCC intermediate representation, which is in several areas higher level than
LLVM-IR, such that several constructs like multi-dimensional arrays are easily
available.  Internally Graphite still uses a rational polyhedral library and
only in some cases relies on an integer linear programming solver.
Graphite uses the classical parallelization detection before code generation and is
not yet closely integrated with the OpenMP code generation. In contrast
to Polly, it has been tested for several years and is reaching production quality.

The only other compiler with an internal polyhedral optimizer we know of is 
IBM XL/C. Unfortunately, we could not find any information on how SCoP detection
and code generation is done.
There exists a variety of source to source transformation tools
such as Pluto \cite{pluto}, PoCC or LooPo.

