\label{chap:background}

\section{Parallelism in programs}
These days it is hard to find somebody using a single-core processor machine.
With the help of multi-core and multi-processor machines it is possible to speed up 
the program by mapping the sections of the program to available processors. This 
is generally termed as parallelism in programs. It is very difficult to parallelize
the entire program though. The degree of parallelism is limited by certain factors which is
explained later in this section. In addition this section discusses various types of parallelism and
make a comparison of various approaches towards parallelism which can be applied to programs.

\subsection{Parallelism and locality}
When there is a need for parallelism there is a need for interprocessor communication.
So while optimizing programs for parallelism extreme attention should be given to
minimize the communication overhead. We can minimize communication if the
processor accesses recently used data. That is we need to improve data
locality. Considering the performance of a single processor it is
essential to extract more data locality which in turn increases
the cache hits. While dealing with parallelism we need to be aware
about the restrictions on the degree of parallelism that can
be extracted from a given program, which is well stated by {\textbf Amdahl}'s law.

Amdahl's law  states that, if \emph{f} is the fraction of the code parallelized, and if
the parallelized version runs on a \emph{p}-processor machine with no communication
or parallelization overhead, the speedup is given by,
\begin{center}
$\frac{1}{(1\ -\ f)\ +\ (f/p)}$
\end{center}

For instance, if half the computation is sequential, the computation can only
double in speed, regardless of the number of processors used. The speedup
is a factor of 1.6 if we have 4 processors. So researchers keep on working
for extracting more parallelism and thereby reducing the fraction of sequential
computation.

\subsection{Realizing parallelism}

Some of the approaches to realize parallelism are explained in this section.

\noindent
\textbf{POSIX Threads/Pthreads}

\noindent
Pthreads provides a standard interface for performing multihreaded computation. 
Threads are subprocesses running with in a process. 
We can find many applications such as a web browser which can take advantage of multithreading.
The efficiency of an application improves when it is designed with threads because they have their
own stack and status. The overhead of creating a separate process can be avoided here.
Resources like files are shared among threads. Though Pthreads are good alternatives for
having multiple processes in a single processor machine it is very difficult to scale
it to multi-core processors. Another limitation of Pthreads is programmers are required to
deal with a lot of thread-specific code. The number of threads required for a computation
need to be hard corded which makes it less scalable.

%\textbf{MPI:}

\noindent
\textbf{OpenMP}

\noindent
In view of the shortcomings of POSIX threads there was an urge to formulate a new threading
interface. The major objective was to overcome the burden of learning different ways for programming threads in different
operating systems with in different programming languages. OpenMP is able to deal with this
by a great extend. As the framework is evolved rather than its APIs, support for pragmas became the distinguished
feature of OpenMP. The user has to specify only the blocks of code that need to be run
as parallel. The compiler does the rest. It will take care of making the pragma annotated blocks into
threads. Necessary APIs are inserted to map those threads into different cores. The example below
shows usage of pragma.

{\footnotesize
\begin{lstlisting}
  #pragma omp parallel for
  for (i = 1; i <= N; i++)
      A[i] = B[i] + C[i]
\end{lstlisting}
}

Another characteristic of OpenMP is that by disabling support for OpenMP the same program can be treated as
single threaded. This enables easy debugging and makes the programmer's life easier.

If the developer needs more fine-grained control a small set of APIs are available in OpenMP. But in this case Pthreads
could be the right choice because it provides a greater number of primitive functions. So if in applications
in which threads require individual attention the appropriate choice would be Pthreads.

Ample care should be taken to ensure the correctness of the program while using OpenMP pragmas. The following
example illustrates that.
{\footnotesize
\begin{lstlisting}
  for (i = 0; i < 10; i++) {
    #pragma omp parallel for private(k)
    for(j = 0; j < 10; j++) {
      k++;
      A[i] += k;
    }
  }
\end{lstlisting}
}

We get incorrect result if the data sharing attribute for the variable \emph{k} is \emph {private}. It should
be \emph{shared} to get the intended result.

%\textbf{OpenCL}

%\textbf{Intel TBB:}

\section{Auto parallelization}
The techniques described in the previous section relies heavily on manually
identifying parallelism, which is not always a good approach.
We can take the advantage of hardware support for parallelism only if the compiler has
support for generating the parallel code. There are interfaces like OpenMP for
developing parallel applications. But the user has to manually provide the annotations
for it in the source code. This becomes a tedious task for the user and he has to
ensure the correctness of the code too. This prompted researchers to explore
mechanisms for finding out the parallel portions of the code without manual intervention.

It can be noticed that most of the execution time of a program is spend inside some
for loop. Parallelizing compiler tries to split up a loop so that its iterations can
be executed on separate processors concurrently. A dependency analysis pass is 
performed on the code to determine whether it can be safely parallelized. The following
example illustrates this.

{\footnotesize
\begin{lstlisting}
  for (i = 1; i <= N; i++)
      A[i] = B[i] + C[i]
\end{lstlisting}
}

\noindent
The analysis detects that there is no dependency between two consecutive iterations and
can be safely parallelized. Consider another example

{\footnotesize
\begin{lstlisting}
  for (i = 2; i <= N; i++)
      A[i] = A[i-1] * 2;
\end{lstlisting}
}

\noindent
Here a particular iteration is dependent on previous one and so its not safe to parallelize.
An intelligent compiler can convert this into parallel as follows.

{\footnotesize
\begin{lstlisting}
  for (i = 1; i <= N; i++)
      A[i] = A[1] * 2 ** (i - 1);
\end{lstlisting}
}

Detecting this kind of opportunities for parallelization and applying automatic transformation
is a tedious task for existing compilers. A powerful mathematical model explained in the next
section act as a helping hand for the compilers to do such transformations with some
restrictions applied on the input.

\section{The polyhedral model}

In this model the program is transformed into an algebraic representation which can be used to
detect data dependences. This representation is then converted in such a way that the degree
of parallelism is improved. Polyhedral optimizations are used for many kind of memory access optimization by
looking into the memory access pattern of any piece of code. Any kind of classical
 loop optimization techniques like tiling can be used for this purpose. The model is
explained in detail in Chapter ~\ref{chap:background}.

\section{LLVM}
LLVM defines a common, low-level code representation in Static Single Assignment
(SSA) form, with several novel features. The LLVM compiler framework and code
representation together provide a combination of key capabilities that are
important for practical, lifelong analysis and transformation of programs.
One of the important features of LLVM is that the output of all the
transformation passes have same intermediate representation(LLVM IR), which
makes the programmer to analyze it with ease.

\section{Polly and OpenMP code generation}
The framework for automatic OpenMP code generation is implemented using,
an open source\footnote{\url{http://llvm.org/releases/2.8/LICENSE.TXT}} compiler  optimization framework that uses a mathematical
 representation, the polyhedral model, to represent and transform loops and other
 control flow structures. It is an effort towards achieving autoparallelism in programs.
 The transformations are being implemented in LLVM(Low level virtual machine). 
Polly can detect parallel loops, issue vector instructions and generate OpenMP code 
corresponding to those loops. Polly try to expose more parallelism
with the help of polyhedral model. A loop which does not look parallel can be transformed
to a parallel loop and these can be vectorized or parallelize using OpenMP. More details on
LLVM and Polly can be found in Chapter  ~\ref{chap:polly}.

\section{Outline of report}
The organization of this report is as described here. In Chapter ~\ref{chap:background} we
describe the background required for understanding the polyhedral model. Chapter ~\ref{chap:polly}
deals with the internals of Polly - Polyhedral optimization in LLVM. Next chapter consists of
the details of the workdone for OpenMP code generation in Polly. Then  Chapter ~\ref{chap:testing}
explains the testing framework and shows the experimental results. And the last chapter
has the list of future projects that can be done on Polly and we conclude with that.

