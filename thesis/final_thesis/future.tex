\label{chap:future}

\section{Conclusion}
Polly integrated with automatic OpenMP code generation helps users to
realize parallelism without much worries about the internals of the program.
Lot of effort and programming time can be saved because
the approach eliminates the need for manually finding parallelism and providing annotation.
Since the optimizations are performed on LLVM-IR the framework is not restricted to only C/C++, but al-
so supports a wide range of other programming languages.
Powerful polyhedral techniques are available to optimize programs, but their impact is still limited. The main reasons
are difficulties to optimize existing programs automatically and generate parallel code. Polly and its integrated OpenMP support
is an attempt to strengthen the impact. There is enough space for further enhancements
and anybody interested can make their contribution. 
There are lot of possibilities for imporving Polly. Some of the subprojects
that can be done are discussed here.

\section{Support for memory access transformations in Polly}
An improvement that can be made to polly is to add support for memory access transformations in Polly.
In many cases it would be great to change the pattern of memory access to obtain better data locality.
This can remove dependences that would otherwise block transformations and it can allow LLVM to use registers to store such values.

Polly performs its optimization on LLVM-IR based on the polyhedral model. Currently the transformations can be applied on Schedule (Order of computations)
Transformations can also be applied on the Memory Access (Pattern of memory access). A proper memory access transformation can improve data locality. This will in turn improve parallelism.

\section{Increasing coverage of Polly}

Polly (Polyhedral optimization framework in LLVM) is showing very nice results for
several testcases. Yet, lot of larger test cases needs to be improved. we can explore
the reasons for this, find solutions for those and implement it. There are two parts for this.

\subsection{Increasing SCoP coverage}

The number of SCoPs detected need to be improved. This can be called as "Increasing SCoP Coverage". 

Expressions like min, max, sext, zext, trunc or unsigned comparisons in the loop bounds or memory
accesses are not handled in the current implementation. For example consider the following loop.
{\footnotesize
\begin{lstlisting}
for (int i = 0; i < N; i++)
  A[i] = B[i] + C[i];
\end{lstlisting}
}
In this case a sext is necessary if the code is translated to LLVM-IR and keep i as an i32 and
use an i64 to calculate the access to A[i]. This is  not currently handled in Polly.

Overflows NSW(No signed wrap) or NUW(No unsigned wrap) are not handled in the current implementation. So
it is not safe to compile a large project with Polly.

Polly can be tested with large benchmarks like SPEC and there is a very high possibility
for finding areas which are not detected as SCoPs. It will be interesting to
explore the reasons for this fix it.
\subsection{Increasing the system coverage}

Some of the testcases are failing when Polly is tested in machines which does not
have 64-bit Operating system. This needs to be fixed and can be called as "Increasing the System Coverage".
This can also be treated as porting to Polly to more architectures.
A solution for this issue could be to derive the data type needed by the maximal possible value a variable can have.

\section{Integrating profile guided optimization into Polly}
An improvement that can be made to Polly is integrating profile guided optimization
\cite{pgo}. The idea is explained below with a few examples. Consider the following code.
{\footnotesize
\begin{lstlisting}
scanf("%d", &b);
for(i = 0; i < N; i +=  b) {
    body;
}
\end{lstlisting}
}
Polly will not detect this as a SCoP because the variable b is read as a user
input. So to detect this as a SCoP we instrument the IR with the information
provided by profiling. Suppose using profiling we figure out that most of the 
time the value of b is say 2. we can convert the above code as follows.
%\begin{verbatim}
{\footnotesize
\begin{lstlisting}
scanf("%d", &b);
if(b == 2) {
  for(i = 0; i < N; i += 2) {
      body;
  }
} else {
    for(i = 0; i < N; i += b) {
        body;
    }
}
\end{lstlisting}
}
Now with the transformed code the for loop inside 'if' will be detected as a 
SCoP and can be parallelised. Since value of b is 2 most of the time, the 
overall performance will be improved.

\noindent
Consider another scenario.
{\footnotesize
\begin{lstlisting}
  for(i = 0; i < N; i++) {
      body;
  }
\end{lstlisting}
}
Suppose using profiling we know that N is always very small. So there will not be
much gain from parallelising it. So we have to tell polly that do not detect
this as a SCoP if N is less than a specific value.
Integrating such versioning we can expect to get heavily optimized performance 
for some often used cases.


